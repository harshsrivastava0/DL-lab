{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10589693,"sourceType":"datasetVersion","datasetId":6553833}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pickle\nimport os\n\n# Function to unpickle CIFAR-10 files\ndef unpickle(file):\n    with open(file, 'rb') as fo:\n        data_dict = pickle.load(fo, encoding='bytes')\n    return data_dict\n\n# Define the path where your CIFAR-10 files are stored\ndata_path = '/kaggle/input/cifar10/cifar-10-batches-py'  # Adjust if necessary\n\n# Load all training batches\ndef load_cifar10(data_path):\n    train_data = []\n    train_labels = []\n    \n    # Load all batch files\n    for i in range(1, 6):\n        batch_file = os.path.join(data_path, f'data_batch_{i}')\n        batch = unpickle(batch_file)\n        train_data.append(batch[b'data'])\n        train_labels.extend(batch[b'labels'])\n\n    # Combine all batches\n    train_data = np.vstack(train_data).reshape(-1, 3, 32, 32).astype(\"float32\")\n    train_labels = np.array(train_labels)\n\n    # Load test batch\n    test_batch = unpickle(os.path.join(data_path, 'test_batch'))\n    test_data = test_batch[b'data'].reshape(-1, 3, 32, 32).astype(\"float32\")\n    test_labels = np.array(test_batch[b'labels'])\n\n    # Load label names\n    meta = unpickle(os.path.join(data_path, 'batches.meta'))\n    label_names = meta[b'label_names']\n    label_names = [label.decode('utf-8') for label in label_names]\n\n    return train_data, train_labels, test_data, test_labels, label_names\n\n# Load the CIFAR-10 data\ntrain_data, train_labels, test_data, test_labels, label_names = load_cifar10(data_path)\n\n# Verify the data\nprint(f\"Training Data Shape: {train_data.shape}\")\nprint(f\"Training Labels Shape: {train_labels.shape}\")\nprint(f\"Test Data Shape: {test_data.shape}\")\nprint(f\"Test Labels Shape: {test_labels.shape}\")\nprint(f\"Label Names: {label_names}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-01-30T06:15:23.736347Z","iopub.execute_input":"2025-01-30T06:15:23.736694Z","iopub.status.idle":"2025-01-30T06:15:24.220624Z","shell.execute_reply.started":"2025-01-30T06:15:23.736665Z","shell.execute_reply":"2025-01-30T06:15:24.219844Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Training Data Shape: (50000, 3, 32, 32)\nTraining Labels Shape: (50000,)\nTest Data Shape: (10000, 3, 32, 32)\nTest Labels Shape: (10000,)\nLabel Names: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\n\n# Normalize the data and convert to tensors\nfrom torchvision.transforms import ToPILImage\n\n# Define data augmentation for training\ntransform_train = transforms.Compose([\n    transforms.ToPILImage(),  # Convert numpy array to PIL Image\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))  # CIFAR-10 mean and std\n])\n\n# Define transformations for testing\ntransform_test = transforms.Compose([\n    transforms.ToPILImage(),  # Convert numpy array to PIL Image\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n])\n\n\n# Custom PyTorch Dataset class\nclass CIFAR10Dataset(Dataset):\n    def __init__(self, data, labels, transform=None):\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img = self.data[idx].transpose(1, 2, 0)  # Convert to HWC format for transformations\n        label = self.labels[idx]\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, label\n\n# Create datasets\ntrain_dataset = CIFAR10Dataset(train_data, train_labels, transform=transform_train)\ntest_dataset = CIFAR10Dataset(test_data, test_labels, transform=transform_test)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-30T06:15:30.041201Z","iopub.execute_input":"2025-01-30T06:15:30.041489Z","iopub.status.idle":"2025-01-30T06:15:30.049856Z","shell.execute_reply.started":"2025-01-30T06:15:30.041468Z","shell.execute_reply":"2025-01-30T06:15:30.048899Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2025-01-30T06:15:34.144990Z","iopub.execute_input":"2025-01-30T06:15:34.145278Z","iopub.status.idle":"2025-01-30T06:15:34.149565Z","shell.execute_reply.started":"2025-01-30T06:15:34.145255Z","shell.execute_reply":"2025-01-30T06:15:34.148577Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class FineTunedResNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(FineTunedResNet, self).__init__()\n        \n        # Load ResNet-18 Pretrained Model\n        self.resnet = models.resnet18(pretrained=True)\n        \n        # Modify first conv layer (ResNet expects 224x224 images, but CIFAR-10 is 32x32)\n        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        \n        # Freeze ResNet layers except the final FC layer\n        for param in self.resnet.parameters():\n            param.requires_grad = False  # Freeze\n        \n        # Replace classifier with LeakyCNN's fully connected layers\n        self.resnet.fc = nn.Sequential(\n            nn.Linear(512, 512),\n            nn.LeakyReLU(0.01),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)  # CIFAR-10 has 10 classes\n        )\n\n    def forward(self, x):\n        return self.resnet(x)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T06:15:37.660468Z","iopub.execute_input":"2025-01-30T06:15:37.660769Z","iopub.status.idle":"2025-01-30T06:15:37.666297Z","shell.execute_reply.started":"2025-01-30T06:15:37.660747Z","shell.execute_reply":"2025-01-30T06:15:37.665355Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision.models as models  # Import this line to access ResNet\nimport time\n\n# Initialize the model (fine-tuned ResNet-18)\nmodel = FineTunedResNet(num_classes=10)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Step 1: Train only the FC layer first\noptimizer = optim.Adam(model.resnet.fc.parameters(), lr=0.001)\n\ndef train_one_epoch(model, train_loader, optimizer, criterion, device):\n    model.train()  # Set model to training mode\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()  # Zero the parameter gradients\n        \n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()  # Backpropagate the loss\n        \n        optimizer.step()  # Update parameters\n\n        running_loss += loss.item()\n\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(train_loader)\n    accuracy = 100 * correct / total\n    return epoch_loss, accuracy\n\n# Step 2: Fine-tune the entire model (unfreeze ResNet)\ndef unfreeze_and_finetune(model):\n    # Unfreeze all layers in ResNet\n    for param in model.resnet.parameters():\n        param.requires_grad = True\n\n    # Re-initialize the optimizer to include all parameters\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Lower learning rate for fine-tuning\n    return optimizer\n\n# Training function with two phases\ndef train_model(model, train_loader, val_loader, epochs=30):\n    global optimizer  # Use the global optimizer\n\n    best_acc = 0.0\n    for epoch in range(epochs):\n        start_time = time.time()\n\n        # Train the model (Feature extraction phase)\n        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n        \n        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%')\n\n        # After a few epochs, unfreeze ResNet and fine-tune (Full fine-tuning phase)\n        if epoch == 5:  # Unfreeze after 5 epochs of training only the FC layer\n            optimizer = unfreeze_and_finetune(model)\n\n        # Optionally evaluate on validation set here\n\n        print(f'Time taken for epoch {epoch + 1}: {time.time() - start_time:.2f} seconds')\n\n# Train the model\ntrain_model(model, train_loader, test_loader, epochs=30)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T06:15:41.761274Z","iopub.execute_input":"2025-01-30T06:15:41.761567Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/30], Loss: 2.0373, Accuracy: 25.49%\nTime taken for epoch 1: 16.38 seconds\nEpoch [2/30], Loss: 1.9709, Accuracy: 28.34%\nTime taken for epoch 2: 16.35 seconds\nEpoch [3/30], Loss: 1.9504, Accuracy: 28.78%\nTime taken for epoch 3: 15.92 seconds\nEpoch [4/30], Loss: 1.9416, Accuracy: 29.12%\nTime taken for epoch 4: 16.38 seconds\nEpoch [5/30], Loss: 1.9364, Accuracy: 29.32%\nTime taken for epoch 5: 16.42 seconds\nEpoch [6/30], Loss: 1.9309, Accuracy: 29.90%\nTime taken for epoch 6: 16.67 seconds\nEpoch [7/30], Loss: 1.4155, Accuracy: 49.44%\nTime taken for epoch 7: 25.05 seconds\nEpoch [8/30], Loss: 1.0440, Accuracy: 63.85%\nTime taken for epoch 8: 24.94 seconds\nEpoch [9/30], Loss: 0.8717, Accuracy: 70.09%\nTime taken for epoch 9: 24.82 seconds\nEpoch [10/30], Loss: 0.7568, Accuracy: 74.16%\nTime taken for epoch 10: 24.75 seconds\nEpoch [11/30], Loss: 0.6698, Accuracy: 77.18%\nTime taken for epoch 11: 24.78 seconds\nEpoch [12/30], Loss: 0.6052, Accuracy: 79.23%\nTime taken for epoch 12: 24.83 seconds\nEpoch [13/30], Loss: 0.5590, Accuracy: 81.05%\nTime taken for epoch 13: 24.61 seconds\nEpoch [14/30], Loss: 0.5133, Accuracy: 82.49%\nTime taken for epoch 14: 24.69 seconds\nEpoch [15/30], Loss: 0.4777, Accuracy: 83.68%\nTime taken for epoch 15: 24.95 seconds\nEpoch [16/30], Loss: 0.4581, Accuracy: 84.50%\nTime taken for epoch 16: 24.86 seconds\nEpoch [17/30], Loss: 0.4250, Accuracy: 85.50%\nTime taken for epoch 17: 24.70 seconds\nEpoch [18/30], Loss: 0.4013, Accuracy: 86.36%\nTime taken for epoch 18: 25.33 seconds\nEpoch [19/30], Loss: 0.3788, Accuracy: 86.73%\nTime taken for epoch 19: 24.93 seconds\nEpoch [20/30], Loss: 0.3530, Accuracy: 87.74%\nTime taken for epoch 20: 24.89 seconds\nEpoch [22/30], Loss: 0.3190, Accuracy: 89.08%\nTime taken for epoch 22: 24.85 seconds\nEpoch [23/30], Loss: 0.3056, Accuracy: 89.35%\nTime taken for epoch 23: 24.75 seconds\nEpoch [24/30], Loss: 0.2889, Accuracy: 90.02%\nTime taken for epoch 24: 24.63 seconds\nEpoch [25/30], Loss: 0.2775, Accuracy: 90.46%\nTime taken for epoch 25: 24.48 seconds\nEpoch [26/30], Loss: 0.2707, Accuracy: 90.50%\nTime taken for epoch 26: 24.85 seconds\nEpoch [27/30], Loss: 0.2539, Accuracy: 91.21%\nTime taken for epoch 27: 24.77 seconds\nEpoch [28/30], Loss: 0.2426, Accuracy: 91.56%\nTime taken for epoch 28: 24.83 seconds\n","output_type":"stream"}],"execution_count":null}]}