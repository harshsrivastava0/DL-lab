{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11177005,"sourceType":"datasetVersion","datasetId":6976062}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport re\nimport random\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T17:46:03.881421Z","iopub.execute_input":"2025-03-26T17:46:03.881841Z","iopub.status.idle":"2025-03-26T17:46:08.195659Z","shell.execute_reply.started":"2025-03-26T17:46:03.881790Z","shell.execute_reply":"2025-03-26T17:46:08.194455Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/poems-100/poems-100.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Load the dataset\npoems_df = pd.read_csv(r'/kaggle/input/poems-100/poems-100.csv')\ntext_data = ' '.join(poems_df['text'].tolist())\n\n# Basic preprocessing\ndef preprocess_text(text):\n    # Remove special characters and extra whitespace\n    text = re.sub(r'[^\\w\\s]', ' ', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text.lower()\n\nprocessed_text = preprocess_text(text_data)\n\n# Tokenize the text into words\nwords = processed_text.split()\nprint(f\"Total words in the dataset: {len(words)}\")\nprint(f\"Unique words: {len(set(words))}\")\n\n# Create vocabulary\nvocab = sorted(set(words))\nword_to_idx = {word: i for i, word in enumerate(vocab)}\nidx_to_word = {i: word for i, word in enumerate(vocab)}\nvocab_size = len(vocab)\n# print(f\"word_to_idx: {word_to_idx}\")\nprint(f\"Vocabulary size: {vocab_size}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T17:47:56.537477Z","iopub.execute_input":"2025-03-26T17:47:56.537847Z","iopub.status.idle":"2025-03-26T17:47:56.577434Z","shell.execute_reply.started":"2025-03-26T17:47:56.537794Z","shell.execute_reply":"2025-03-26T17:47:56.576495Z"}},"outputs":[{"name":"stdout","text":"Total words in the dataset: 25508\nUnique words: 5158\nVocabulary size: 5158\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Create sequences for training\ndef create_sequences(words, seq_length):\n    sequences = []\n    targets = []\n    for i in range(0, len(words) - seq_length):\n        seq = words[i:i+seq_length]\n        target = words[i+seq_length]\n        sequences.append([word_to_idx[word] for word in seq])\n        targets.append(word_to_idx[target])\n    return np.array(sequences), np.array(targets)\n\nseq_length = 10  # Length of input sequences\nsequences, targets = create_sequences(words, seq_length)\n\n# Convert to one-hot encoding\ndef to_one_hot(idx, vocab_size):\n    one_hot = np.zeros((len(idx), vocab_size))\n    for i, idx_val in enumerate(idx):\n        one_hot[i, idx_val] = 1\n    return one_hot\n\n# Create PyTorch dataset\nclass PoemDataset(Dataset):\n    def __init__(self, sequences, targets, vocab_size, one_hot=True):\n        self.sequences = sequences\n        self.targets = targets\n        self.vocab_size = vocab_size\n        self.one_hot = one_hot\n        \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        if self.one_hot:\n            sequence = to_one_hot(self.sequences[idx], self.vocab_size)\n            sequence = torch.FloatTensor(sequence)\n        else:\n            sequence = torch.LongTensor(self.sequences[idx])\n        \n        target = torch.LongTensor([self.targets[idx]])\n        return sequence, target\n\n# Create dataset and dataloader\none_hot_dataset = PoemDataset(sequences, targets, vocab_size, one_hot=True)\none_hot_dataloader = DataLoader(one_hot_dataset, batch_size=64, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T17:58:08.591859Z","iopub.execute_input":"2025-03-26T17:58:08.592216Z","iopub.status.idle":"2025-03-26T17:58:08.789861Z","shell.execute_reply.started":"2025-03-26T17:58:08.592188Z","shell.execute_reply":"2025-03-26T17:58:08.788759Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class SimpleRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x, hidden=None):\n        if hidden is None:\n            batch_size = x.size(0)\n            hidden = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n        \n        output, hidden = self.rnn(x, hidden)\n        output = self.fc(output[:, -1, :])  # Take only the last output\n        return output, hidden\n    \n    def init_hidden(self, batch_size):\n        return torch.zeros(1, batch_size, self.hidden_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T17:59:11.817593Z","iopub.execute_input":"2025-03-26T17:59:11.817999Z","iopub.status.idle":"2025-03-26T17:59:11.824917Z","shell.execute_reply.started":"2025-03-26T17:59:11.817964Z","shell.execute_reply":"2025-03-26T17:59:11.823775Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class SimpleLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleLSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x, hidden=None):\n        if hidden is None:\n            batch_size = x.size(0)\n            h0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n            c0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n            hidden = (h0, c0)\n        \n        output, hidden = self.lstm(x, hidden)\n        output = self.fc(output[:, -1, :])  # Take only the last output\n        return output, hidden\n    \n    def init_hidden(self, batch_size):\n        h0 = torch.zeros(1, batch_size, self.hidden_size)\n        c0 = torch.zeros(1, batch_size, self.hidden_size)\n        return (h0, c0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T17:59:41.369861Z","iopub.execute_input":"2025-03-26T17:59:41.370251Z","iopub.status.idle":"2025-03-26T17:59:41.377764Z","shell.execute_reply.started":"2025-03-26T17:59:41.370221Z","shell.execute_reply":"2025-03-26T17:59:41.376725Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def train_model(model, dataloader, epochs=10, learning_rate=0.001):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for sequences, targets in dataloader:\n            sequences = sequences.to(device)\n            targets = targets.squeeze().to(device)\n            \n            # Forward pass\n            outputs, _ = model(sequences)\n            loss = criterion(outputs, targets)\n            \n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n        \n        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}')\n    \n    return model\n\n# Initialize and train the RNN model\ninput_size = vocab_size  # One-hot encoding size\nhidden_size = 128\noutput_size = vocab_size  # Predict the next word\n\nrnn_model = SimpleRNN(input_size, hidden_size, output_size)\nrnn_model = train_model(rnn_model, one_hot_dataloader, epochs=20)\n\n# Initialize and train the LSTM model\nlstm_model = SimpleLSTM(input_size, hidden_size, output_size)\nlstm_model = train_model(lstm_model, one_hot_dataloader, epochs=20)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:00:11.876433Z","iopub.execute_input":"2025-03-26T18:00:11.876813Z","iopub.status.idle":"2025-03-26T18:27:45.033722Z","shell.execute_reply.started":"2025-03-26T18:00:11.876783Z","shell.execute_reply":"2025-03-26T18:27:45.032692Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20, Loss: 6.9503\nEpoch 2/20, Loss: 6.4194\nEpoch 3/20, Loss: 6.1346\nEpoch 4/20, Loss: 5.7807\nEpoch 5/20, Loss: 5.4190\nEpoch 6/20, Loss: 5.0581\nEpoch 7/20, Loss: 4.6954\nEpoch 8/20, Loss: 4.3294\nEpoch 9/20, Loss: 3.9682\nEpoch 10/20, Loss: 3.6035\nEpoch 11/20, Loss: 3.2522\nEpoch 12/20, Loss: 2.8995\nEpoch 13/20, Loss: 2.5615\nEpoch 14/20, Loss: 2.2402\nEpoch 15/20, Loss: 1.9360\nEpoch 16/20, Loss: 1.6645\nEpoch 17/20, Loss: 1.4184\nEpoch 18/20, Loss: 1.1947\nEpoch 19/20, Loss: 1.0092\nEpoch 20/20, Loss: 0.8424\nEpoch 1/20, Loss: 6.9700\nEpoch 2/20, Loss: 6.5289\nEpoch 3/20, Loss: 6.3518\nEpoch 4/20, Loss: 6.0960\nEpoch 5/20, Loss: 5.7648\nEpoch 6/20, Loss: 5.3879\nEpoch 7/20, Loss: 4.9665\nEpoch 8/20, Loss: 4.4995\nEpoch 9/20, Loss: 4.0220\nEpoch 10/20, Loss: 3.5475\nEpoch 11/20, Loss: 3.0834\nEpoch 12/20, Loss: 2.6302\nEpoch 13/20, Loss: 2.2041\nEpoch 14/20, Loss: 1.8147\nEpoch 15/20, Loss: 1.4671\nEpoch 16/20, Loss: 1.1623\nEpoch 17/20, Loss: 0.9135\nEpoch 18/20, Loss: 0.7041\nEpoch 19/20, Loss: 0.5429\nEpoch 20/20, Loss: 0.4149\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def generate_text(model, seed_text, max_length=100, temperature=1.0):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    model.eval()\n    \n    words = seed_text.lower().split()\n    current_words = words[-seq_length:] if len(words) > seq_length else words\n    \n    # Pad if necessary\n    if len(current_words) < seq_length:\n        current_words = ['the'] * (seq_length - len(current_words)) + current_words\n    \n    # Convert to indices\n    current_indices = [word_to_idx.get(word, 0) for word in current_words]\n    \n    generated_words = list(current_words)\n    \n    with torch.no_grad():\n        for _ in range(max_length):\n            # Convert to one-hot\n            x = to_one_hot(current_indices, vocab_size)\n            x = torch.FloatTensor(x).unsqueeze(0).to(device)\n            \n            # Forward pass\n            output, _ = model(x)\n            \n            # Apply temperature\n            output = output.div(temperature)\n            \n            # Sample from the output distribution\n            probs = torch.softmax(output, dim=1).cpu().data.numpy().ravel()\n            idx = np.random.choice(len(probs), p=probs)\n            \n            # Get the predicted word\n            word = idx_to_word[idx]\n            generated_words.append(word)\n            \n            # Update current indices\n            current_indices = current_indices[1:] + [idx]\n    \n    return ' '.join(generated_words)\n\n# Generate text with RNN\nseed_text = \"the rose is red\"\ngenerated_text_rnn = generate_text(rnn_model, seed_text, max_length=50)\nprint(\"RNN Generated Text:\")\nprint(generated_text_rnn)\n\n# Generate text with LSTM\ngenerated_text_lstm = generate_text(lstm_model, seed_text, max_length=50)\nprint(\"\\nLSTM Generated Text:\")\nprint(generated_text_lstm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:28:36.643930Z","iopub.execute_input":"2025-03-26T18:28:36.644465Z","iopub.status.idle":"2025-03-26T18:28:36.943790Z","shell.execute_reply.started":"2025-03-26T18:28:36.644436Z","shell.execute_reply":"2025-03-26T18:28:36.942767Z"}},"outputs":[{"name":"stdout","text":"RNN Generated Text:\nthe the the the the the the rose is red man voice and home they said air some bring you because they fair not nothing nothing cents coarse it flight something you ll here give thou separate be bear are in bosom and dead one s skeptic ten sweet and yet hour the morning and resign them and still my\n\nLSTM Generated Text:\nthe the the the the the the rose is red roar the lo the rest under the fish in the crowd from the lives long still children thee my thee o just no flag ever loving way clothes my blow my yonder walls on the ears properties my bare scares my tread around in beat hush a green d upon\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Create dataset without one-hot encoding\nembedding_dataset = PoemDataset(sequences, targets, vocab_size, one_hot=False)\nembedding_dataloader = DataLoader(embedding_dataset, batch_size=64, shuffle=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RNNWithEmbedding(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n        super(RNNWithEmbedding, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x, hidden=None):\n        embedded = self.embedding(x)\n        \n        if hidden is None:\n            batch_size = x.size(0)\n            hidden = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n        \n        output, hidden = self.rnn(embedded, hidden)\n        output = self.fc(output[:, -1, :])  # Take only the last output\n        return output, hidden\n    \n    def init_hidden(self, batch_size):\n        return torch.zeros(1, batch_size, self.hidden_size)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LSTMWithEmbedding(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n        super(LSTMWithEmbedding, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x, hidden=None):\n        embedded = self.embedding(x)\n        \n        if hidden is None:\n            batch_size = x.size(0)\n            h0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n            c0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n            hidden = (h0, c0)\n        \n        output, hidden = self.lstm(embedded, hidden)\n        output = self.fc(output[:, -1, :])  # Take only the last output\n        return output, hidden\n    \n    def init_hidden(self, batch_size):\n        h0 = torch.zeros(1, batch_size, self.hidden_size)\n        c0 = torch.zeros(1, batch_size, self.hidden_size)\n        return (h0, c0)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize and train the RNN model with embeddings\nembedding_dim = 100\nhidden_size = 128\noutput_size = vocab_size\n\nrnn_embedding_model = RNNWithEmbedding(vocab_size, embedding_dim, hidden_size, output_size)\nrnn_embedding_model = train_model(rnn_embedding_model, embedding_dataloader, epochs=20)\n\n# Initialize and train the LSTM model with embeddings\nlstm_embedding_model = LSTMWithEmbedding(vocab_size, embedding_dim, hidden_size, output_size)\nlstm_embedding_model = train_model(lstm_embedding_model, embedding_dataloader, epochs=20)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_text_with_embedding(model, seed_text, max_length=100, temperature=1.0):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    model.eval()\n    \n    words = seed_text.lower().split()\n    current_words = words[-seq_length:] if len(words) > seq_length else words\n    \n    # Pad if necessary\n    if len(current_words) < seq_length:\n        current_words = ['the'] * (seq_length - len(current_words)) + current_words\n    \n    # Convert to indices\n    current_indices = [word_to_idx.get(word, 0) for word in current_words]\n    \n    generated_words = list(current_words)\n    \n    with torch.no_grad():\n        for _ in range(max_length):\n            # Convert to tensor\n            x = torch.LongTensor(current_indices).unsqueeze(0).to(device)\n            \n            # Forward pass\n            output, _ = model(x)\n            \n            # Apply temperature\n            output = output.div(temperature)\n            \n            # Sample from the output distribution\n            probs = torch.softmax(output, dim=1).cpu().data.numpy().ravel()\n            idx = np.random.choice(len(probs), p=probs)\n            \n            # Get the predicted word\n            word = idx_to_word[idx]\n            generated_words.append(word)\n            \n            # Update current indices\n            current_indices = current_indices[1:] + [idx]\n    \n    return ' '.join(generated_words)\n\n# Generate text with RNN + Embedding\nseed_text = \"the rose is red\"\ngenerated_text_rnn_emb = generate_text_with_embedding(rnn_embedding_model, seed_text, max_length=50)\nprint(\"RNN + Embedding Generated Text:\")\nprint(generated_text_rnn_emb)\n\n# Generate text with LSTM + Embedding\ngenerated_text_lstm_emb = generate_text_with_embedding(lstm_embedding_model, seed_text, max_length=50)\nprint(\"\\nLSTM + Embedding Generated Text:\")\nprint(generated_text_lstm_emb)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport time\n\n# Function to measure training time and loss\ndef measure_performance(model_class, dataloader, epochs=10):\n    start_time = time.time()\n    losses = []\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model_class.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for sequences, targets in dataloader:\n            sequences = sequences.to(device)\n            targets = targets.squeeze().to(device)\n            \n            # Forward pass\n            outputs, _ = model(sequences)\n            loss = criterion(outputs, targets)\n            \n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n        \n        avg_loss = total_loss / len(dataloader)\n        losses.append(avg_loss)\n        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n    \n    training_time = time.time() - start_time\n    return model, losses, training_time\n\n# Compare training time and loss\nmodels = {\n    'RNN (One-Hot)': rnn_model,\n    'LSTM (One-Hot)': lstm_model,\n    'RNN (Embedding)': rnn_embedding_model,\n    'LSTM (Embedding)': lstm_embedding_model\n}\n\n# Plot loss comparison\nplt.figure(figsize=(10, 6))\nfor model_name, model in models.items():\n    plt.plot(range(1, 21), model.losses, label=model_name)\n\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss Comparison')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Compare generated text quality\nseed_texts = [\"the rose is red\", \"i love the way\", \"she walks in beauty\"]\nfor seed in seed_texts:\n    print(f\"\\nSeed: '{seed}'\")\n    for model_name, model in models.items():\n        if 'Embedding' in model_name:\n            generated = generate_text_with_embedding(model, seed, max_length=30)\n        else:\n            generated = generate_text(model, seed, max_length=30)\n        print(f\"{model_name}: {generated}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}